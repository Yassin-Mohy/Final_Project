# -*- coding: utf-8 -*-
"""Final_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wqxdCU-boO_FOU-uroLkoV-DRcDQEe35
"""

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns

df = pd.read_csv('telecom_churn.csv')

df.shape

df.head()

df.columns

# removing not needed columns
df = df[[ 'telecom_partner', 'gender', 'age', 'state',
        'date_of_registration', 'num_dependents', 'estimated_salary',
        'calls_made', 'sms_sent', 'data_used', 'churn']].copy()

# renaming columns
df = df.rename(columns={'telecom_partner':'Telecom_Partner',
                        'gender':'Gender',
                        'age':'Age',
                        'state':'State',
                        'date_of_registration':'Registration_Date',
                        'num_dependents':'Num_Dependents',
                        'estimated_salary':'Estimated_Salary',
                        'calls_made':'Calls_Made',
                        'sms_sent':'SMS_Sent',
                        'data_used':'Data_Used',
                        'churn':'Churn'})

df.describe()

# checking nulls
df.isna().sum()

# checking duplicates
df.duplicated().sum()

# number of churns
df['Churn'].value_counts()

# percentage of churns
df['Churn'].value_counts(normalize=True) * 100

(df['Calls_Made'] < 0).sum()

(df['SMS_Sent'] < 0).sum()

(df['Data_Used'] < 0).sum()

# handling negative values
df['Calls_Made'] = df['Calls_Made'].apply(lambda x: max(x,0))
df['SMS_Sent'] = df['SMS_Sent'].apply(lambda x: max(x,0))
avg = df[df['Data_Used'] >= 0]['Data_Used'].mean()
df['Data_Used'] = df['Data_Used'].apply(lambda x: avg if x < 0 else x)

df.corr(numeric_only=True)

plt.figure(figsize=(10,8))
sns.heatmap(df.corr(numeric_only=True),annot=True)
plt.title("Feature Correlation")
plt.show()

df.info()

df['Registration_Date'] = pd.to_datetime(df['Registration_Date'])

# Split age into 4 groups
df['age_group'] = pd.qcut(df['Age'], q=4, labels=['Q1 (18–36)', 'Q2 (37–48)', 'Q3 (49–60)', 'Q4 (61+)'])
df['age_group'].value_counts()

df.groupby('age_group')['Churn'].mean().reset_index()

sns.barplot(x='age_group', y='Churn', data=df)
plt.title('Churn Rate by Age Quantile')
plt.show()

df.drop('Age',axis=1,inplace=True)

df['Gender'].value_counts()

churn_by_gender = df.groupby('Gender')['Churn'].mean() * 100
print(churn_by_gender)

sns.barplot(x=churn_by_gender.index, y=churn_by_gender.values)
plt.title('Churn Rate by Gender')
plt.ylabel('Churn Rate (%)')
plt.xlabel('Gender')
plt.show()

churn_by_partner = df.groupby('Telecom_Partner')['Churn'].mean() * 100
print(churn_by_partner)

sns.barplot(x=churn_by_partner.index, y=churn_by_partner.values)
plt.title('Churn Rate by Telecom Partner')
plt.ylabel('Churn Rate (%)')
plt.xlabel('Telecom Partner')
plt.show()

churn_by_state = df.groupby('State')['Churn'].mean() * 100

top10_states= churn_by_state.sort_values(ascending=False).head(10)
print(top10_states)

# Plot the top 10 states with highest churn rate

sns.barplot(x=top10_states.values, y=top10_states.index)
plt.title('Top 10 States with Highest Churn Rate')
plt.xlabel('Churn Rate (%)')
plt.ylabel('State')
plt.show()

df['Tenure'] = (pd.to_datetime('today') - df['Registration_Date']) / pd.Timedelta(days=30)

sns.boxplot(x='Churn',y='Tenure',data=df)
plt.title('Tenure by Churn')
plt.show()

df.drop('Registration_Date',axis=1,inplace=True)

df.groupby('Churn')['Estimated_Salary'].mean()

sns.boxplot(x='Churn',y='Estimated_Salary',data=df)
plt.title('Churn by Estimated Salary')
plt.show()

df['salary_level'] = pd.cut(df['Estimated_Salary'], bins=3,labels=['Low','Medium','High'])
churn_by_salary = df.groupby('salary_level')['Churn'].mean().reset_index()
print(churn_by_salary)

df.drop('Estimated_Salary',axis=1,inplace=True)

df.groupby('Churn')['Calls_Made'].mean()

sns.kdeplot(data=df,x='Calls_Made',hue='Churn',fill=True)
plt.title('Churn by Calls Made')
plt.show()

df.groupby('Churn')['SMS_Sent'].mean()

sns.boxplot(x='Churn',y='SMS_Sent',data=df)
plt.title('Churn by SMS Sent')
plt.show()

df.groupby('Churn')['Data_Used'].mean()

sns.boxplot(x='Churn',y='Data_Used',data=df)
plt.title('Churn by Data Used')
plt.show()

"""# Machine Learning Model using Scikit Learn"""

# One-Hot Encode nominal variables
df_encoded = pd.get_dummies(df, columns=['Telecom_Partner', 'Gender', 'State'], drop_first=True)

from sklearn.preprocessing import LabelEncoder

# Label Encode ordinal variables
label_encoders = {}
for col in ['age_group', 'salary_level']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

from sklearn.model_selection import train_test_split

# Separate features and target variable
X = df_encoded.drop(columns=['Churn'])
y = df_encoded['Churn']

# Split the data into train & test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

# Train Logistic Regression
logreg = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')
logreg.fit(X_train, y_train)

# Evaluate on test set
y_pred = logreg.predict(X_test)
print(classification_report(y_test, y_pred))

roc_auc = roc_auc_score(y_test, logreg.predict_proba(X_test)[:, 1])
print(f"ROC-AUC Score: {roc_auc:.4f}")

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score

# Train Random Forest with class weights
rf = RandomForestClassifier(random_state=42, n_estimators=100, class_weight='balanced')
rf.fit(X_train, y_train)

# Evaluate on test set
y_pred_rf = rf.predict(X_test)
print("Random Forest Classification Report:")
print(classification_report(y_test, y_pred_rf))

roc_auc_rf = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])
print(f"Score: {roc_auc_rf:.4f}")

# Evaluate on training set
y_train_pred_rf = rf.predict(X_train)
print("Random Forest Classification Report (Training Set):")
print(classification_report(y_train, y_train_pred_rf))

roc_auc_train_rf = roc_auc_score(y_train, rf.predict_proba(X_train)[:, 1])
print(f"ROC-AUC Score (Training Set): {roc_auc_train_rf:.4f}")

"""# Model performs better with training data than test data, there is Overfitting"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
# Train Random Forest with class weights
rf = RandomForestClassifier(random_state=42,
                            n_estimators=50 # Reduce number of trees
                            ,max_depth=10, # Limit tree depth
                            min_samples_split=10, # Require more samples per split
                            class_weight='balanced')
rf.fit(X_train, y_train)

# Evaluate on test set
y_pred_rf = rf.predict(X_test)
print("Random Forest Classification Report:")
print(classification_report(y_test, y_pred_rf))

roc_auc_rf = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])
print(f"Score: {roc_auc_rf:.4f}")

# Evaluate on training set
y_train_pred_rf = rf.predict(X_train)
print("Random Forest Classification Report (Training Set):")
print(classification_report(y_train, y_train_pred_rf))

roc_auc_train_rf = roc_auc_score(y_train, rf.predict_proba(X_train)[:, 1])
print(f"ROC-AUC Score (Training Set): {roc_auc_train_rf:.4f}")

"""# Overfitting issue is solved, but the accuracy not that high"""

rf = RandomForestClassifier(
    random_state=42,
    n_estimators=200,          # Increase number of trees
    max_depth=20,              # Increase maximum depth
    min_samples_split=2,       # Decrease minimum samples required to split
    min_samples_leaf=1,        # Decrease minimum samples required at a leaf node
    class_weight='balanced'    # Handle class imbalance
)
rf.fit(X_train, y_train)

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

rf = RandomForestClassifier(random_state=42, class_weight='balanced')
rf.fit(X_train_resampled, y_train_resampled)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
numerical_features = ['Calls_Made', 'SMS_Sent', 'Data_Used']
X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])
X_test[numerical_features] = scaler.transform(X_test[numerical_features])

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5],
    'class_weight': ['balanced']
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    scoring='roc_auc',
    cv=3,
    verbose=1,
    n_jobs=-1
)
grid_search.fit(X_train, y_train)

best_rf = grid_search.best_estimator_

from sklearn.metrics import roc_auc_score

y_pred_rf = rf.predict(X_test)
print("Random Forest Classification Report:")
print(classification_report(y_test, y_pred_rf))
roc_auc = roc_auc_score(y_test, best_rf.predict_proba(X_test)[:, 1])
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Evaluate on training set
y_train_pred_rf = rf.predict(X_train)
print("Random Forest Classification Report (Training Set):")
print(classification_report(y_train, y_train_pred_rf))

roc_auc_train_rf = roc_auc_score(y_train, rf.predict_proba(X_train)[:, 1])
print(f"ROC-AUC Score (Training Set): {roc_auc_train_rf:.4f}")

"""# Deployment using Streamlit"""

pip install streamlit

# Save the Trained Model
import joblib
joblib.dump(best_rf, 'churn_model.pkl')

import streamlit as st
import pandas as pd
import joblib

# Load the trained model
model = joblib.load('churn_model.pkl')

# Title and navigation
st.title("Churn Prediction Dashboard")
option = st.sidebar.radio("Select an option:", ["Exploratory Data Analysis (EDA)", "Churn Prediction"])

# Churn Prediction Section
if option == "Churn Prediction":
    st.header("Churn Prediction")
    age = st.number_input("Age", min_value=18, max_value=100, value=35)
    estimated_salary = st.number_input("Estimated Salary", min_value=0, value=50000)
    calls_made = st.number_input("Calls Made", min_value=0, value=100)
    sms_sent = st.number_input("SMS Sent", min_value=0, value=50)
    data_used = st.number_input("Data Used (GB)", min_value=0, value=5)

    if st.button("Predict Churn"):
        input_data = pd.DataFrame({
            'age': [age],
            'estimated_salary': [estimated_salary],
            'calls_made': [calls_made],
            'sms_sent': [sms_sent],
            'data_used': [data_used]
        })
        prediction = model.predict(input_data)[0]
        probability = model.predict_proba(input_data)[0][1]

        if prediction == 1:
            st.error("The customer is likely to churn.")
        else:
            st.success("The customer is not likely to churn.")
        st.write(f"Probability of churn: {probability:.2%}")

